1. 本次爬虫使用了正则表达式去解析网页，而非使用beautifulsoup等解析工具解析网页
2. 使用了简单的判断和粗暴的异常处理，直接使用RequestException进行异常捕捉
3. 将字典转化为json字符串进行存储，这过程种可能存在编码问题
4. 用字典方式存储数据时，使用了生成器创建字典
5. 最后简单使用了线程池实现多线程处理，因为存在并发，所以使用多线程的存储结果可能乱序，
   而使用单线程时，文件中的存储结果为顺序
